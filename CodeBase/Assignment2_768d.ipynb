{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d6030ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76063ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Installing and importing libraries\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Installing and importing libraries\")\n",
    "!pip install -qU pymilvus milvus-lite ragas sentence-transformers transformers torch datasets pandas evaluate \"pymilvus[milvus_lite]\"\n",
    "!pip install -qU langchain # For the text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4726028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers, torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730e4fc",
   "metadata": {},
   "source": [
    "Run-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483bb780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up configuration\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up configuration\")\n",
    "EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "COLLECTION_NAME = 'rag_mini_768d'\n",
    "DB_NAME = \"rag_experiments_768d.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006a7471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preparing data\n"
     ]
    }
   ],
   "source": [
    "#Data Loading\n",
    "print(\"\\nLoading and preparing data\")\n",
    "passages = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed09ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3200 documents into 4430 chunks.\n"
     ]
    }
   ],
   "source": [
    "#Chunking the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = []\n",
    "for index, row in passages.iterrows():\n",
    "    text_chunks = text_splitter.split_text(row['passage'])\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        chunks.append({'id': f\"{row.name}_{i}\", 'passage': chunk})\n",
    "chunk_df = pd.DataFrame(chunks)\n",
    "print(f\"Split {len(passages)} documents into {len(chunk_df)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Step 4: Generating embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314d9a6b645d4c85a9dd6e388f12c055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd96d2eb6db9402abf7118fe7e4e13cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be006a65eeb4dfd814b5dec9d2d87e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5125d36a364ffaa31542cfc132faab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab1d49c31694b77aa8208a37375d53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c776463c6a24207a1ffdc11a6cb087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f6d4baa25c41b08d409760627cfd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe28dd862b24885ae2f03fd01b90d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879a06d14d39485eb45bb2936eb3910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca09ceefd034f0c837f4ee375ab201d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673b1d452491401ab602b771323ea19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4b6df41a64488eb3728ca3be3e7d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4430 embeddings of dimension 768.\n"
     ]
    }
   ],
   "source": [
    "#Embedding Generation\n",
    "print(\"\\nGenerating embeddings\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embeddings = embedding_model.encode(chunk_df['passage'].tolist(), show_progress_bar=True)\n",
    "chunk_df['embedding'] = list(embeddings)\n",
    "EMBEDDING_DIM = embeddings.shape[1]\n",
    "print(f\"Generated {len(embeddings)} embeddings of dimension {EMBEDDING_DIM}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e0393",
   "metadata": {},
   "source": [
    "Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e63a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up Milvus vector database\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nSetting up Milvus vector database\")\n",
    "# # Defining Schema\n",
    "# id_field = FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=256)\n",
    "# passage_field = FieldSchema(name=\"passage\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "# embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM)\n",
    "# schema = CollectionSchema(fields=[id_field, passage_field, embedding_field], description=\"RAG Wikipedia Collection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ea50b",
   "metadata": {},
   "source": [
    "Creating Client and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b44076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# client = MilvusClient(DB_NAME)\n",
    "# if client.has_collection(collection_name=COLLECTION_NAME):\n",
    "#     print(f\"Collection '{COLLECTION_NAME}' already exists. Dropping it.\")\n",
    "#     client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "# client.create_collection(collection_name=COLLECTION_NAME, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 4430 entities.\n"
     ]
    }
   ],
   "source": [
    "# #Inserting Data into Milvus\n",
    "# data_to_insert = chunk_df.to_dict(orient='records')\n",
    "# res = client.insert(collection_name=COLLECTION_NAME, data=data_to_insert)\n",
    "# print(f\"Inserted {res['insert_count']} entities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus setup complete and collection is loaded.\n"
     ]
    }
   ],
   "source": [
    "# #Creating\n",
    "# index_params = client.prepare_index_params()\n",
    "# index_params.add_index(field_name=\"embedding\", index_type=\"AUTOINDEX\", metric_type=\"L2\")\n",
    "# client.create_index(collection_name=COLLECTION_NAME, index_params=index_params)\n",
    "# client.load_collection(collection_name=COLLECTION_NAME)\n",
    "# print(\"Milvus setup complete and collection is loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConnecting to existing database\")\n",
    "client = MilvusClient(DB_NAME)\n",
    "if not client.has_collection(collection_name=COLLECTION_NAME):\n",
    "    raise ValueError(f\"Collection '{COLLECTION_NAME}' not found. Please run the `build_database.ipynb` notebook first.\")\n",
    "\n",
    "client.load_collection(collection_name=COLLECTION_NAME)\n",
    "print(f\"Successfully connected to collection '{COLLECTION_NAME}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8946e2",
   "metadata": {},
   "source": [
    "RAG Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "014adef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up RAG pipeline components\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up RAG pipeline components\")\n",
    "#QA Dataset and Metric\n",
    "queries_df = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\").head(25)\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a485243",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19969bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Strategies\n",
    "def create_instruction_prompt(context, query): return f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "def create_cot_prompt(context, query): return f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer: Let's think step by step.\"\n",
    "def create_persona_prompt(context, query): return f\"You are an expert encyclopedia. Answer the question based on the context.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8695f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Function\n",
    "def generate_rag_response(query, prompt_strategy, top_k):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    search_results = client.search(collection_name=COLLECTION_NAME, data=[query_embedding], limit=top_k, output_fields=[\"passage\"])\n",
    "    context = \"\\\\n\".join([hit['entity']['passage'] for hit in search_results[0]])\n",
    "    prompt = prompt_strategy(context, query)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d620aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing evaluation loop\n",
      "--- Running: top_k=1, prompt='Instruction' ---\n",
      "--- Running: top_k=1, prompt='Chain-of-Thought' ---\n",
      "--- Running: top_k=1, prompt='Persona' ---\n",
      "--- Running: top_k=3, prompt='Instruction' ---\n",
      "--- Running: top_k=3, prompt='Chain-of-Thought' ---\n",
      "--- Running: top_k=3, prompt='Persona' ---\n",
      "--- Running: top_k=5, prompt='Instruction' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running: top_k=5, prompt='Chain-of-Thought' ---\n",
      "--- Running: top_k=5, prompt='Persona' ---\n"
     ]
    }
   ],
   "source": [
    "#Evaluation Function\n",
    "print(\"\\nExecuting evaluation loop\")\n",
    "results = []\n",
    "top_k_options = [1, 3, 5]\n",
    "prompt_strategies = {\n",
    "    \"Instruction\": create_instruction_prompt,\n",
    "    \"Chain-of-Thought\": create_cot_prompt,\n",
    "    \"Persona\": create_persona_prompt,\n",
    "}\n",
    "\n",
    "for k in top_k_options:\n",
    "    for name, func in prompt_strategies.items():\n",
    "        print(f\"--- Running: top_k={k}, prompt='{name}' ---\")\n",
    "        predictions_text = [generate_rag_response(q, func, k) for q in queries_df['question']]\n",
    "        predictions = [{'prediction_text': p, 'id': str(i)} for i, p in enumerate(predictions_text)]\n",
    "        references = [{'answers': {'text': [gt], 'answer_start': [0]}, 'id': str(i)} for i, gt in enumerate(queries_df['answer'])]\n",
    "        metrics = squad_metric.compute(predictions=predictions, references=references)\n",
    "        results.append({\n",
    "            \"Embedding Model\": EMBEDDING_MODEL_NAME,\n",
    "            \"Embedding Size\": EMBEDDING_DIM,\n",
    "            \"Prompt Strategy\": name,\n",
    "            \"Top K\": k,\n",
    "            \"Exact Match\": metrics['exact_match'],\n",
    "            \"F1 Score\": metrics['f1']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5900b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying results and cleaning up\n",
      "\n",
      "\n",
      "--- FINAL RESULTS ---\n",
      "| Embedding Model   |   Embedding Size | Prompt Strategy   |   Top K |   Exact Match |   F1 Score |\n",
      "|:------------------|-----------------:|:------------------|--------:|--------------:|-----------:|\n",
      "| all-mpnet-base-v2 |              768 | Instruction       |       1 |            64 |    69.7734 |\n",
      "| all-mpnet-base-v2 |              768 | Chain-of-Thought  |       1 |             0 |    14.2056 |\n",
      "| all-mpnet-base-v2 |              768 | Persona           |       1 |            64 |    69.0462 |\n",
      "| all-mpnet-base-v2 |              768 | Instruction       |       3 |            68 |    77.0462 |\n",
      "| all-mpnet-base-v2 |              768 | Chain-of-Thought  |       3 |             0 |    11.3789 |\n",
      "| all-mpnet-base-v2 |              768 | Persona           |       3 |            72 |    81.0462 |\n",
      "| all-mpnet-base-v2 |              768 | Instruction       |       5 |            72 |    80.7128 |\n",
      "| all-mpnet-base-v2 |              768 | Chain-of-Thought  |       5 |             0 |    10.958  |\n",
      "| all-mpnet-base-v2 |              768 | Persona           |       5 |            72 |    79.7128 |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDisplaying results and cleaning up\")\n",
    "client.close()\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\n--- FINAL RESULTS ---\")\n",
    "print(results_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68c45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"runs/Assignment2_768d_results.csv\", index=False)\n",
    "with open(\"runs/Assignment2_768d_results.md\",\"w\") as f:\n",
    "    f.write(results_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8a8d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THE QUERY\n",
      "'Was Abraham Lincoln the sixteenth President of the United States?'\n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      "'yes'\n",
      "\n",
      "RETRIEVED CONTEXT (Top 1 Chunks)\n",
      "\n",
      "[CHUNK 1]:\n",
      "\"Abraham Lincoln (February 12, 1809 Ã¢Â€Â“ April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination. As an outspoken opponent of the expansion of slavery in the United States, \"[I]n his short autobiography written for the 1860 presidential campaign, Lincoln would describe his protest in the Illinois legislature as one that 'briefly defined his position on the slavery question, and so far as it goes, it was then the same that it is now.\" This was in\"\n",
      "\n",
      "FINAL GENERATED ANSWER\n",
      "'yes'\n"
     ]
    }
   ],
   "source": [
    "def show_single_query_breakdown(index_to_test):\n",
    "\n",
    "    client = MilvusClient(DB_NAME)\n",
    "    client.load_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "    question = queries_df.iloc[index_to_test]['question']\n",
    "    ground_truth = queries_df.iloc[index_to_test]['answer']\n",
    "    top_k = 1\n",
    "    \n",
    "    def create_persona_prompt(context, query):\n",
    "        return f\"You are an expert encyclopedia. Answer the question based on the context.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "\n",
    "    print(f\"\\nTHE QUERY\\n'{question}'\")\n",
    "    print(f\"\\nGROUND TRUTH ANSWER:\\n'{ground_truth}'\")\n",
    "\n",
    "    #Retrieval Step\n",
    "    query_embedding = embedding_model.encode(question)\n",
    "    search_results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=[query_embedding],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"passage\"]\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = [hit['entity']['passage'] for hit in search_results[0]]\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    print(f\"\\nRETRIEVED CONTEXT (Top {top_k} Chunks)\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        print(f\"\\n[CHUNK {i+1}]:\\n\\\"{chunk}\\\"\")\n",
    "\n",
    "    prompt = create_persona_prompt(context, question)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=128)\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nFINAL GENERATED ANSWER\")\n",
    "    print(f\"'{generated_answer}'\")\n",
    "\n",
    "\n",
    "show_single_query_breakdown(index_to_test=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7957c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
